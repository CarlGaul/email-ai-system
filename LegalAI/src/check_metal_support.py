#!/usr/bin/env python3
import subprocess
import sys
import time
import psutil

def check_ollama_metal_support():
    """Check if Ollama supports Metal acceleration"""
    print("ğŸ” OLLAMA METAL ACCELERATION CHECK")
    print("-" * 40)
    
    try:
        # Check Ollama version and capabilities
        result = subprocess.run(["ollama", "--version"], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(f"âœ… Ollama Version: {result.stdout.strip()}")
        else:
            print("âŒ Ollama not found or not responding")
            return False
        
        # Test inference speed with Metal (if available)
        print("\nğŸ§ª Testing Qwen inference speed...")
        start_time = time.time()
        
        test_result = subprocess.run([
            "ollama", "run", "qwen2.5:14b", 
            "Respond with exactly: 'Metal acceleration test complete'"
        ], capture_output=True, text=True, timeout=120)
        
        inference_time = time.time() - start_time
        
        if test_result.returncode == 0:
            print(f"âœ… Qwen inference completed in {inference_time:.2f} seconds")
            if inference_time < 30:
                print("ğŸš€ Excellent performance - likely using Metal acceleration")
            elif inference_time < 60:
                print("âš¡ Good performance - Metal may be active")
            else:
                print("âš ï¸  Slower performance - Metal may not be optimized")
            return True
        else:
            print(f"âŒ Qwen inference failed: {test_result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("â° Ollama test timed out")
        return False
    except Exception as e:
        print(f"âŒ Error testing Ollama: {e}")
        return False

def check_pytorch_metal():
    """Check PyTorch Metal (MPS) availability"""
    print("\nğŸ” PYTORCH METAL (MPS) CHECK")
    print("-" * 40)
    
    try:
        import torch
        
        if torch.backends.mps.is_available():
            print("âœ… PyTorch Metal (MPS) is available")
            
            if torch.backends.mps.is_built():
                print("âœ… PyTorch was built with Metal support")
            else:
                print("âš ï¸  PyTorch not built with Metal support")
            
            # Test Metal performance
            print("\nğŸ§ª Testing Metal tensor operations...")
            device = torch.device("mps")
            
            start_time = time.time()
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            torch.mps.synchronize()
            metal_time = time.time() - start_time
            
            print(f"âœ… Metal tensor operation: {metal_time:.4f} seconds")
            
            # Compare with CPU
            start_time = time.time()
            x_cpu = torch.randn(1000, 1000)
            y_cpu = torch.randn(1000, 1000)
            z_cpu = torch.mm(x_cpu, y_cpu)
            cpu_time = time.time() - start_time
            
            print(f"ğŸ“Š CPU tensor operation: {cpu_time:.4f} seconds")
            print(f"ğŸš€ Metal speedup: {cpu_time / metal_time:.2f}x faster")
            
            return True
        else:
            print("âŒ PyTorch Metal (MPS) not available")
            return False
            
    except ImportError:
        print("âŒ PyTorch not installed")
        return False
    except Exception as e:
        print(f"âŒ Error testing PyTorch Metal: {e}")
        return False

def check_coreml_support():
    """Check Core ML availability"""
    print("\nğŸ” CORE ML CHECK")
    print("-" * 40)
    
    try:
        import coremltools as ct
        print(f"âœ… Core ML Tools available: {ct.__version__}")
        
        # Test basic Core ML functionality
        import numpy as np
        
        # Create a simple model for testing
        print("ğŸ§ª Testing Core ML model creation...")
        
        # Simple linear model test
        from coremltools.models import MLModel
        from coremltools.models.utils import save_spec
        
        print("âœ… Core ML basic functionality working")
        return True
        
    except ImportError:
        print("âŒ Core ML Tools not installed")
        print("ğŸ’¡ Install with: pip3 install coremltools")
        return False
    except Exception as e:
        print(f"âŒ Error testing Core ML: {e}")
        return False

def check_system_resources():
    """Check system memory and CPU"""
    print("\nğŸ” SYSTEM RESOURCES")
    print("-" * 40)
    
    # Memory check
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ Total RAM: {memory.total / (1024**3):.1f} GB")
    print(f"ğŸ’¾ Available RAM: {memory.available / (1024**3):.1f} GB")
    print(f"ğŸ’¾ Memory usage: {memory.percent}%")
    
    if memory.available < 8 * (1024**3):
        print("âš ï¸  Less than 8GB available - may limit AI model performance")
    else:
        print("âœ… Sufficient memory for AI operations")
    
    # CPU check
    cpu_count = psutil.cpu_count()
    cpu_freq = psutil.cpu_freq()
    
    print(f"ğŸ–¥ï¸  CPU cores: {cpu_count}")
    if cpu_freq:
        print(f"ğŸ–¥ï¸  CPU frequency: {cpu_freq.current:.0f} MHz")
    
    # Quick CPU test
    print("ğŸ§ª Testing CPU performance...")
    start_time = time.time()
    result = sum(i * i for i in range(1000000))
    cpu_time = time.time() - start_time
    print(f"âœ… CPU test completed in {cpu_time:.4f} seconds")
    
    return memory.available > 6 * (1024**3)

def main():
    """Run comprehensive hardware compatibility check"""
    print("ğŸ” PHASE 2: HARDWARE COMPATIBILITY ASSESSMENT")
    print("=" * 60)
    
    results = {
        'ollama_metal': check_ollama_metal_support(),
        'pytorch_metal': check_pytorch_metal(),
        'coreml': check_coreml_support(),
        'system_resources': check_system_resources()
    }
    
    print("\n" + "=" * 60)
    print("ğŸ“Š COMPATIBILITY SUMMARY")
    print("=" * 60)
    
    for check, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {check.replace('_', ' ').title()}: {status}")
    
    # Recommendations
    print("\nğŸ’¡ OPTIMIZATION RECOMMENDATIONS:")
    
    if results['ollama_metal']:
        print("  âœ… Ollama ready for optimization")
    else:
        print("  ğŸ”§ Ollama needs configuration for Metal acceleration")
    
    if results['pytorch_metal']:
        print("  âœ… Ready for PyTorch Metal acceleration")
    else:
        print("  ğŸ”§ Install PyTorch with Metal support")
    
    if results['coreml']:
        print("  âœ… Core ML ready for Legal-BERT optimization")
    else:
        print("  ğŸ”§ Install Core ML Tools: pip3 install coremltools")
    
    if results['system_resources']:
        print("  âœ… System resources sufficient")
    else:
        print("  ğŸ”§ Consider closing applications to free memory")
    
    all_ready = all(results.values())
    
    print(f"\nğŸ¯ PHASE 2 READINESS: {'âœ… READY' if all_ready else 'âš ï¸  NEEDS WORK'}")
    
    return all_ready

if __name__ == "__main__":
    main()
